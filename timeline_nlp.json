
{
    "title": {
        "media": {
          "url": "https://drive.google.com/file/d/1PYy6MK8fzyp3jLZjPTej4COxRu37b3ti/view?usp=sharing"
        },
        "text": {
          "headline": "  여행스타그램",
          "text": "<p>#해시태그 #기반 #관광 #여행 #추천 </p>"
        }
    },
    "events": [
      {
        "media": {
          "url":"https://drive.google.com/file/d/140D2-rjGiaLeB8SPc-O1UOTVXoiR-oXQ/view?usp=sharing"
        },
        "start_date": {
          "year" : 0,
          "display_date" : " "
        },
        "text": {
          "headline": "여행스타그램을 만들게 된 이유",
          "text": "<br/><p>여행을 떠날 때 만나게 되는 고민을 해결하기 위해 사람들은 인터넷 검색을 통해 가려는 장소에 대한 정보를 얻습니다. 그러나 일반적인 검색으로는 알맞은 정보가 나오지 않거나 신뢰성이 떨어지는 경우가 존재합니다. 알맞은 정보가 나오는 경우에도 가독성이 떨어져 한눈에 알아보기 어려운 경우가 있습니다. 이러한 형상을 보고 “해시태그”라는 단어를 떠올렸습니다. 현재 우리가 먹고, 놀고, 사용하는 것들에는 키워드가 존재합니다. 원하는 장소의 키워드를 해시태그에 넣어 사람들이 만족할 수 있는 여행을 추천합니다.</p><br/>"
        }
      },
      {
        "media": {
          "url":"https://drive.google.com/file/d/1xpwX0GkOP8Uat4P78dkG9VZU7rCaTBcR/view?usp=sharing"
        },
        "start_date": {
          "year" : 10,
          "display_date" : " "
        },
        "text": {
          "headline": "여행스타그램 사용 방법 - 1",
          "text": "<br /><p>아무것도 입력하지 않은 상태의 페이지입니다. 이때의 화면 비율은 80% 권장합니다. <br /><br /> ① 원하는 여행 장소와 키워드를 #을 붙여 입력합니다. 이때 여행 장소는 필수로 입력해야 하고, #과 키워드 입력 후에 띄어쓰기를 해야 합니다. 키워드에 제한 수는 없으나 많을수록 시간이 더 소요될 수 있습니다. 키워드 없이 장소만 검색도 가능합니다.<br /> 예시 - #곡성 #힐링 #휴식 / #서울 #행복 / #강남구 #여행 / #통영  <br /><br /> ②ChatGPT 추천수 선택합니다. <br /><br /> ③추천 받을 항목을 체크합니다. (최소 1 최대 8) <br /><br /> ④슬라이드 바를 통해 조회할 문서의 개수를 조정합니다. <br /><br /> ⑤Submit 버튼을 눌러서 완료합니다. <br /> </p><br/>"
        }
      },
      {
        "media": {
          "url":"https://drive.google.com/file/d/1cGKGFvOVGvoXJvBwxleRGdaoSKy4mq-R/view?usp=sharing"
        },
        "start_date": {
          "year" : 20,
          "display_date" : " "
        },
        "text": {
          "headline": "여행스타그램 사용 방법 - 2",
          "text": "<br /><p> 예시 - #곡성 #힐링 #휴식 /ChatGPT 추천수 3 / 추천 받을 항목: 관광지 / 슬라이드 바: 5로 했을 경우의 화면입니다. <br /><br /> ①오른쪽 공백란에 관광지 추천 결과가 생깁니다. ∨화살표 버튼을 눌러, 5가지 검색된 문서 중 3가지의 ChatGPT 추천 결과를 확인할 수 있습니다. <br /> * ChatGPT 추천수와 슬라이드 바 수를 입력했더라도, 실제로 존재하는 API데이터 수가 적어 입력한 수보다 조회 결과값이 적게 나오는 경우도 존재합니다. <br /> * ∨화살표 버튼를 누른 후의 화면에서 입력했던 ChatGPT 추천수대로 지도·ChatGPT의 추천 이유·추천된 장소의 이미지를 확인해볼 수 있습니다.<br /><br /> ②-1 원하는 결과가 나왔다면 ▶버튼을 누릅니다. <br /> -▶버튼을 눌렀을 때 흰 화면은 로딩 페이지이므로, 잠시 대기 부탁드립니다.<br /><br /> ②-2 새로운 정보를 검색하고 싶다면, Refresh 버튼을 누르고 이전의 절차를 밟아 결과를 확인할 수 있습니다.<br /> </p><br/>"
        }
      },
      {
        "media": {
          "url":"https://drive.google.com/file/d/1QNU1_aZ6qacrSIaNYefxs5KSkBZFAf0l/view?usp=sharing"
        },
        "start_date": {
          "year" : 30,
          "display_date" : " "
        },
        "text": {
          "headline": "여행스타그램 사용 방법 - 3",
          "text": "<br /><p> 로딩될 때 흰 화면이 나오지 않고 이 화면이 나올 경우 재시작 부탁드립니다.</p><br/>"
        }
      },
      {
        "media": {
          "url":"https://drive.google.com/file/d/1lc06m9txmBEtASFdI7PfWdN2F_mKDBsT/view?usp=sharing"
        },
        "start_date": {
          "year" : 40,
          "display_date" : " "
        },
        "text": {
          "headline": "여행스타그램 사용 방법 - 4",
          "text": "<br /><p> 로딩이 끝나면 여행스타그램 피드가 정리되어 나타납니다. <br/>- 다시하기 버튼을 누르면 아무것도 입력하지 않은 상태의 페이지로 돌아갑니다. <br/><br/>  여행스타그램 사용법을 숙지 완료했다면, 오른쪽 하단의 ▶버튼을 눌러주십시오.</p><br/>"
        }
      }
    ]
    //   {
    //     "media": {
    //       "url": "https://historyofinformation.com/images/Screen_Shot_2020-09-20_at_10.28.36_AM_big.png",
    //       "caption": "Kathleen Britten, Xenia Sweeting and Andrew Booth working on ARC in December 1946 (<a target=\"_blank\" href='https://www.dcs.bbk.ac.uk/site/assets/files/1029/50yearsofcomputing.pdf'>credits</a>)"
    //     },
    //     "start_date": {
    //       "year": "1948"
    //     },
    //     "text": {
    //       "headline": "First NLP application",
    //       "text": "<p>This is how the story goes:</p><p>Andrew D. Booth who, in 1944, had just gained a PhD studying the crystal structure of explosives and already experimenting with the automation of the large sets of calculations needed to determine crystal structures from X-ray images. Andrew Booth asked if the Rockefeller Foundation would fund a computer for London University. Weaver said that the Rockefeller Foundation could not fund a computer for mathematical calculations but that he had begun to think about using a computer to carry out natural language translation and that the Foundation probably could fund a computer for research in that area.</p><p>Thus Birkbeck University of London became for the next fifteen years a leading centre for natural language research. Initially the tiny memory on computers meant it was very difficult to do any serious processing but Andrew Booth and his research students developed techniques for parsing text and also for building dictionaries.</p>"
    //     }
    //   },
    //   {
    //     "media": {
    //       "url": "https://cdn-images-1.medium.com/max/652/1*8G5Jjppyxy96yGmYk0YwoQ.png",
    //       "caption": "Warren Weaver (<a target=\"_blank\" href='https://cdn-images-1.medium.com/max/652/1*8G5Jjppyxy96yGmYk0YwoQ.png'>credits</a>)"
    //     },
    //     "start_date": {
    //       "year": "1949",
    //       "month": "7"
    //     },
    //     "text": {
    //       "headline": "Weavers Memorandum<br>'Translation'",
    //       "text": "<p>In july 1949 Warren Weaver sent to some 30 acquaintances a memorandum on the possibilities of using the newly invented digital computers on the task of translating documents. Said to be probably the single most influential publication in the early days of machine translation, it formulated goals and methods before most people had any idea of what computers might be capable of.</p>"
    //     }
    //   },
    //   {
    //     "media": {
    //       "url": "https://archive.org/download/MIND--COMPUTING-MACHINERY-AND-INTELLIGENCE/page/cover_t.jpg",
    //       "caption": "Publication of The Imitation Game (<a target=\"_blank\" href='https://archive.org/download/MIND--COMPUTING-MACHINERY-AND-INTELLIGENCE/page/cover_t.jpg'>credits</a>)"
    //     },
    //     "start_date": {
    //       "year": "1950",
    //       "month": "10"
    //     },
    //     "text": {
    //       "headline": "Turing Test",
    //       "text": "<p>The Turing test, originally called The Imitation Game by Alan Turing in 1950, is a test of a machine's ability to exhibit intelligent behaviour equivalent to, or indistinguishable from, that of a human.</p><p>Turing proposed that a human evaluator would judge natural language conversations between a human and a machine designed to generate human-like responses. The evaluator would be aware that one of the two partners in conversation is a machine, and all participants would be separated from one another. The conversation would be limited to a text-only channel such as a computer keyboard and screen so the result would not depend on the machine's ability to render words as speech. If the evaluator cannot reliably tell the machine from the human, the machine is said to have passed the test.</p>"
    //     }
    //   },
    //   {
    //     "media": {
    //       "url": "https://i.pinimg.com/originals/df/10/5c/df105c921e09b6893445a7d875ea7d1d.jpg",
    //       "caption": "The IBM 701 Mainframe Computer (<a target=\"_blank\" href='https://i.pinimg.com/originals/df/10/5c/df105c921e09b6893445a7d875ea7d1d.jpg'>credits</a>)"
    //     },
    //     "start_date": {
    //       "year": "1954",
    //       "month": "1"
    //     },
    //     "text": {
    //       "headline": "The Georgetown–IBM experiment",
    //       "text": "The Georgetown–IBM experiment was an influential demonstration of machine translation, the experiment involved completely automatic translation of more than sixty Russian sentences into English. The authors claimed that within three or five years, machine translation would be a solved problem (J. Hutchins, 2005)"
    //     }
    //   },
    //   {
    //     "media": {
    //       "url": "https://upload.wikimedia.org/wikipedia/commons/f/f0/Chomsky-Syntactic-Structures-Grammar-Model.jpg",
    //       "caption": "The grammar model from Syntactic Structures (1957) (<a target=\"_blank\" href='https://upload.wikimedia.org/wikipedia/commons/f/f0/Chomsky-Syntactic-Structures-Grammar-Model.jpg'>credits</a>)"
    //     },
    //     "start_date": {
    //       "year": "1957",
    //       "month": "2"
    //     },
    //     "text": {
    //       "headline": "Syntactic Structures",
    //       "text": "Noam Chomsky’s Syntactic Structures revolutionized Linguistics with 'Universal Grammar', a rule based system of syntactic structures."
    //     }
    //   },
    //   {
    //     "media": {
    //       "url": "https://upload.wikimedia.org/wikipedia/commons/c/c3/John_Rupert_Firth.png",
    //       "caption": "John Rupert Firth (<a target=\"_blank\" href='https://upload.wikimedia.org/wikipedia/commons/c/c3/John_Rupert_Firth.png'>credits</a>)"
    //     },
    //     "start_date": {
    //       "year": "1957"
    //     },
    //     "text": {
    //       "headline": "'You Shall Know A Word<br>By The Company It Keeps'",
    //       "text": "<p>John Rupert Firth (1890-1960) was an English linguist and a leading figure in British linguistics during the 1950s. Firth is noted for drawing attention to the context-dependent nature of meaning with his notion of 'context of situation', and his work on collocational meaning.</p><p>An influential position in lexical semantics holds that semantic representations for words can be derived through analysis of patterns of lexical co-occurrence in large language corpora. Hence, the quote 'You Shall Know A Word By The Company It Keeps'.</p>"
    //     }
    //   },
    //   {
    //     "media": {
    //       "url": "https://www.channelone.com/wp-content/uploads/2015/03/bigstock-Pile-Of-Words-1896131-crop.jpg",
    //       "caption": "(<a target=\"_blank\" href='http://korpus.uib.no/icame/manuals/BROWN/INDEX.HTM'>credits</a>)"
    //     },
    //     "start_date": {
    //       "year": "1961"
    //     },
    //     "text": {
    //       "headline": "Brown Corpus",
    //       "text": "The Brown Corpus is an electronic collection of text samples of American English, the first major structured corpus of varied genres. This corpus first set the bar for the scientific study of the frequency and distribution of word categories in everyday language use. It consists of 1 million words."
    //     }
    //   },
    //   {
    //     "media": {
    //       "url": "https://www.pbs.org/wgbh/nova/media/images/antarctica-last-1.width-2000.jpg",
    //       "caption": "Antarctica (<a target=\"_blank\" href=''>credits</a>)"
    //     },
    //     "start_date": {
    //       "year": "1966"
    //     },
    //     "text": {
    //       "headline": "NLP Winter",
    //       "text": "<p>Over-promised, under-delivered. A NLP (AI) Winter is a period of reduced funding and interest in NLP. The Automatic Language Processing Advisory Committee (ALPAC) concluded in a famous 1966 report that Machine Translation (MT) was slower, less accurate and twice as expensive as human translation and that 'there is no immediate or predictable prospect of useful MT'. <a target=\"_blank\" href='https://en.wikipedia.org/wiki/AI_winter#Machine_translation_and_the_ALPAC_report_of_1966'>(source)</a></p>"
    //     }
    //   },
    //   {
    //     "media": {
    //       "url": "https://image3.slideserve.com/6919812/examples-with-the-basic-conceptual-dependencies-l.jpg",
    //       "caption": "Conecptual Dependencies (<a target=\"_blank\" href='https://image3.slideserve.com/6919812/examples-with-the-basic-conceptual-dependencies-l.jpg'>credits</a>)"
    //     },
    //     "start_date": {
    //       "year": "1969",
    //       "month": "5"
    //     },
    //     "text": {
    //       "headline": "Conceptual Dependency Theory",
    //       "text": "<p>Roger Schank at Stanford University introduced the model in 1969. Schank developed the model to represent knowledge for natural language input into computers. Partly influenced by the work of Sydney Lamb, his goal was to make the meaning independent of the words used in the input, i.e. two sentences identical in meaning, would have a single representation.</p>"
    //     }
    //   },
    //   {
    //     "media": {
    //       "url": "https://cdn.theatlantic.com/assets/media/img/mt/2014/06/Screen_Shot_2014_06_09_at_12.20.58_PM/lead_large.png",
    //       "caption": "When ELIZA met PARRY (<a target=\"_blank\" href='https://cdn.theatlantic.com/assets/media/img/mt/2014/06/Screen_Shot_2014_06_09_at_12.20.58_PM/lead_large.png'>credits</a>)"
    //     },
    //     "start_date": {
    //       "year": "1972"
    //     },
    //     "text": {
    //       "headline": "Early Chatterbots",
    //       "text": "<p>ELIZA (aka DOCTOR) was created at MIT in 1964 to demonstrate the superficiality of communication between humans and machines, Eliza simulated conversation by using a 'pattern matching' and substitution methodology that gave users an illusion of understanding on the part of the program, but had no built in framework for contextualizing events.</p><p>PARRY was written in 1972 by psychiatrist Kenneth Colby at Stanford. It attempted to simulate a person with paranoid schizophrenia.</p><p>One of the best known demos at the first International Conference on Computer Communications (ICCC 1972) features a conversation where PARRY and ELIZA were hooked up over ARPANET and 'talked' to each other.</p>"
    //     }
    //   },
    //   {
    //     "media": {
    //       "url": "https://images.slideplayer.com/27/9104998/slides/slide_2.jpg",
    //       "caption": "Knowledge organization (<a target=\"_blank\" href='https://images.slideplayer.com/27/9104998/slides/slide_2.jpg'>credits</a>)"
    //     },
    //     "start_date": {
    //       "year": "1975"
    //     },
    //     "text": {
    //       "headline": "Ontology-based Systems",
    //       "text": "<p>During the 1970s many programmers began to write 'conceptual ontologies', which structured real-world information into computer-understandable data. An example is MARGIE (Meaning Analysis Response Generation and Inference on English), a language translation program. The system used only 11 primitive acts as a basis for defining all conceptual relationships in english.</p>"
    //     }
    //   },
    //   {
    //     "media": {
    //       "url": "https://cdn-images-1.medium.com/max/800/1*drXFxQuhg9o-VDXnhE-B3Q.png",
    //       "caption": "WordNet <a target=\"_blank\" href='https://cdn-images-1.medium.com/max/800/1*drXFxQuhg9o-VDXnhE-B3Q.png'>credits</a>"
    //     },
    //     "start_date": {
    //       "year": "1985"
    //     },
    //     "text": {
    //       "headline": "WordNet",
    //       "text": "<p>WordNet is a large lexical database of English. Nouns, verbs, adjectives and adverbs are grouped into sets of cognitive synonyms (117.000 synsets), each expressing a distinct concept. Synsets are interlinked by means of conceptual-semantic and lexical relations. The resulting network of meaningfully related words and concepts can be navigated.</p>"
    //     }
    //   },
    //   {
    //     "media": {
    //       "url": "https://cdn-images-1.medium.com/max/800/1*JMU_Rps2ys6MmAo2UHWaCg.png",
    //       "caption": "Penn Treebank for POS tags <a target=\"_blank\" href='https://cdn-images-1.medium.com/max/800/1*JMU_Rps2ys6MmAo2UHWaCg.png'>credits</a>"
    //     },
    //     "start_date": {
    //       "year": "1989"
    //     },
    //     "text": {
    //       "headline": "Penn Treebank",
    //       "text": "<p>A treebank is a parsed text corpus that annotates syntactic or semantic sentence structure. The construction of parsed corpora in the early 1990s revolutionized computational linguistics, which benefitted from large-scale empirical data. The exploitation of treebank data has been important ever since the first large-scale treebank, The Penn Treebank, was published. </p>"
    //     }
    //   },
    //   {
    //     "media": {
    //       "url": "https://knowledge4policy.ec.europa.eu/sites/default/files/styles/highlight_thumbnail/public/adobestock_111708696.jpeg?itok=6FXz7lJZ",
    //       "caption": "<a target=\"_blank\" href='https://knowledge4policy.ec.europa.eu/sites/default/files/styles/highlight_thumbnail/public/adobestock_111708696.jpeg?itok=6FXz7lJZ'>credits</a>"
    //     },
    //     "start_date": {
    //       "year": "1990"
    //     },
    //     "text": {
    //       "headline": "Large Scale Multi-Lingual Corpora",
    //       "text": "<p>Machine Translation (MT) flourishes in the nineties due to large-scale, multi-lingual corpora from the European union, Canada (french-english) and Japan. Although domain-specific corpora are still needed. <a target=\"_blank\" href='http://www.hutchinsweb.me.uk/Nutshell-2005.pdf'>(source)</a></p>"
    //     }
    //   },
    //   {
    //     "media": {
    //       "url": "https://www.microsoft.com/en-us/research/uploads/prod/2018/10/15.png",
    //       "caption": "EMNLP Conference Top Authors (1996-2018) <a target=\"_blank\" href='https://www.microsoft.com/en-us/research/project/academic/articles/emnlp-conference-analytics/'>credits</a>"
    //     },
    //     "start_date": {
    //       "year": "1996",
    //       "month": "5"
    //     },
    //     "text": {
    //       "headline": "First EMNLP Conference",
    //       "text": "<p>Empirical Methods in Natural Language Processing (EMNLP) is a leading conference in the area of Natural Language Processing. EMNLP is organized by the ACL special interest group on linguistic data (SIGDAT). EMNLP was started in 1996, based on an earlier conference series called Workshop on Very Large Corpora (WVLC).</p>"
    //     }
    //   },
    //   {
    //     "media": {
    //       "url": "https://cdn-images-1.medium.com/max/800/1*68iYn4wq8VpEuXsa4anLig.png",
    //       "caption": "Global spam volume as percentage of total e-mail traffic from 2007 to 2019 (<a target=\"_blank\" href='https://www.statista.com/statistics/420400/spam-email-traffic-share-annual/'>credits</a>)"
    //     },
    //     "start_date": {
    //       "year": "1998"
    //     },
    //     "text": {
    //       "headline": "Spam Detection",
    //       "text": "<p>Naive Bayes was first applied to spam detection in Heckerman et al. (1998). The article addresses the growing problem of junk E-mail on the Internet.</p>"
    //     }
    //   },
    //   {
    //     "media": {
    //       "url": "https://i2.wp.com/charlenecassar.com/wp-content/uploads/2018/09/apache-lucene.png",
    //       "caption": " (<a target=\"_blank\" href='https://i2.wp.com/charlenecassar.com/wp-content/uploads/2018/09/apache-lucene.png'>credits</a>)"
    //     },
    //     "start_date": {
    //       "year": "1999"
    //     },
    //     "text": {
    //       "headline": "Apache Lucene<br>Open-source Search Engine",
    //       "text": "<p>Apache Lucene is an important free and open-source search engine software library. Lucene Core is a Java library providing powerful indexing and search features, as well as spellchecking, hit highlighting and advanced analysis/tokenization capabilities. Several projects extend Lucene's capability, such as Apache SOLR, ElasticSearch and MongoDB.</p>"
    //     }
    //   },
    //   {
    //     "media": {
    //       "url": "https://www.researchgate.net/profile/Andrea_Lekkas/publication/334654187/figure/fig1/AS:784189741871111@1563976812811/Visualization-of-the-Neural-Probabilistic-Language-Model-ideated-by-Bengio-et-al-in.png",
    //       "caption": "A Neural Probabilistic Language Model (<a target=\"_blank\" href='https://www.researchgate.net/publication/2413241_A_Neural_Probabilistic_Language_Model'>credits</a>)"
    //     },
    //     "start_date": {
    //       "year": "2001",
    //       "month": "1"
    //     },
    //     "text": {
    //       "headline": "The First Neural Language Model",
    //       "text": "<p>The first Neural Language Model was introduced by Yoshio Bengio And His Team. They used the feed-forward neural network to introduce the first neural language model. It described an artificial neural network that didn’t use the connection to form a cycle. It’s quite different from the recurrent neural network. In this system, the data moved in one direction from input to output nodes.</p>"
    //     }
    //   },
    //   {
    //     "media": {
    //       "url": "https://www.kdnuggets.com/wp-content/uploads/wikipedia-free-3.jpg",
    //       "caption": "Wikipedia as a Corpus (<a target=\"_blank\" href='https://www.kdnuggets.com/wp-content/uploads/wikipedia-free-3.jpg'>credits</a>)"
    //     },
    //     "start_date": {
    //       "year": "2001",
    //       "month": "1"
    //     },
    //     "text": {
    //       "headline": "Wikipedia as a Corpus",
    //       "text": "<p>Wikipedia is a free, multilingual open-collaborative online encyclopedia. Started in 2001 and has (in 2021) 55 million articles in more than 300 languages. Wikipedia is one of the most popular open data sources for building a corpus for NLP tasks.</p>"
    //     }
    //   },
    //   {
    //     "media": {
    //       "url": "https://images.slideplayer.com/16/5183651/slides/slide_13.jpg",
    //       "caption": "CRF vs HMM (<a target=\"_blank\" href='https://images.slideplayer.com/16/5183651/slides/slide_13.jpg'>credits</a>)"
    //     },
    //     "start_date": {
    //       "year": "2001",
    //       "month": "6"
    //     },
    //     "text": {
    //       "headline": "CRF as alternative for HMM",
    //       "text": "<p>Hidden Markov Models (HMM) and stochastic grammars have been applied to a wide variety of problems in text and speech processing, including topic segmentation, part-of-speech (POS) tagging, information extraction, and syntactic disambiguation. However, it is not practical to represent multiple interacting features or long-range dependencies of the observations. Conditional Random Fields (CRF) was the alternative <a href='https://repository.upenn.edu/cgi/viewcontent.cgi?article=1162&context=cis_papers'>framework</a> for building probabilistic models to segment and label sequence data.</p>"
    //     }
    //   },
    //   {
    //     "media": {
    //       "url": "https://www.researchgate.net/profile/Nianwen_Xue/publication/230876707/figure/fig1/AS:300383344840707@1448628378749/The-OntoNotes-Database-Schema.png",
    //       "caption": "The OntoNotes Database Schema (<a target=\"_blank\" href='https://www.researchgate.net/profile/Nianwen_Xue/publication/230876707/figure/fig1/AS:300383344840707@1448628378749/The-OntoNotes-Database-Schema.png'>credits</a>)"
    //     },
    //     "start_date": {
    //       "year": "2007",
    //       "month": "5"
    //     },
    //     "text": {
    //       "headline": "OntoNotes Release 1.0",
    //       "text": "<p>The OntoNotes project is a collaborative effort to make a resource available to the natural language research community so that decoders can be trained to generate the same structure in new documents. It aims to annotate a large corpus comprising various genres of text (news, conversational telephone speech, weblogs, use net, broadcast, talk shows) in three languages (English, Chinese, and Arabic) with structural information (syntax and predicate argument structure) and shallow semantics (word sense linked to an ontology and coreference).</p>"
    //     }
    //   },
    //   {
    //     "media": {
    //       "url": "https://image.slidesharecdn.com/multitaskfornlp0417-170417064740/95/multitask-learning-for-nlp-2-638.jpg",
    //       "caption": "What is Multitask Learning? (<a target=\"_blank\" href='https://image.slidesharecdn.com/multitaskfornlp0417-170417064740/95/multitask-learning-for-nlp-2-638.jpg'>credits</a>)"
    //     },
    //     "start_date": {
    //       "year": "2008"
    //     },
    //     "text": {
    //       "headline": "Multi-Task Learning",
    //       "text": "<p>Generally, as soon as you find yourself optimizing more than one loss function, you are effectively doing multi-task learning (in contrast to single-task learning).</p>"
    //     }
    //   },
    //   {
    //     "media": {
    //       "url": "https://repository-images.githubusercontent.com/1349775/202c4680-8f7c-11e9-91c6-745fdcbeffe8",
    //       "caption": "Gensim (<a target=\"_blank\" href='https://repository-images.githubusercontent.com/1349775/202c4680-8f7c-11e9-91c6-745fdcbeffe8'>credits</a>)"
    //     },
    //     "start_date": {
    //       "year": "2009"
    //     },
    //     "text": {
    //       "headline": "Gensim",
    //       "text": "<p>Gensim is a popular Python library for topic modelling, document indexing and similarity retrieval with large corpora.</p>"
    //     }
    //   },
    //   {
    //     "media": {
    //       "url": "https://i.stack.imgur.com/0wi0Q.png",
    //       "caption": "NLTK <a target=\"_blank\" href='https://i.stack.imgur.com/0wi0Q.png'>credits</a>"
    //     },
    //     "start_date": {
    //       "year": "2011",
    //       "month": "7"
    //     },
    //     "text": {
    //       "headline": "NLTK",
    //       "text": "<p>Natural Language Toolkit (NLTK) is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries, and an active discussion forum.</p>"
    //     }
    //   },
    //   {
    //     "media": {
    //       "url": "https://www.youtube.com/watch?v=P18EdAKuC1U",
    //       "caption": "Watson and the Jeopardy! Challenge <a target=\"_blank\" href='https://www.youtube.com/watch?v=P18EdAKuC1U'>credits</a>"
    //     },
    //     "start_date": {
    //       "year": "2011",
    //       "month": "2"
    //     },
    //     "text": {
    //       "headline": "IBM Watson wins Jeopardy!",
    //       "text": "<p>Watson is a Question Answering system that won the Jeopardy! contest, defeating the best human players. This was the first man-versus-machine competition in Jeopardy!'s history. Watson won both the first game and the overall match to win the grand prize of $1 million</p>"
    //     }
    //   },
    //   {
    //     "media": {
    //       "url": "https://i.pinimg.com/564x/69/00/55/690055b6e6a1f7b3b6eaf880d03ab9d9.jpg",
    //       "caption": "Siri jokes <a target=\"_blank\" href='https://i.pinimg.com/564x/69/00/55/690055b6e6a1f7b3b6eaf880d03ab9d9.jpg'>credits</a>"
    //     },
    //     "start_date": {
    //       "year": "2011",
    //       "month": "10"
    //     },
    //     "text": {
    //       "headline": "Siri from Apple",
    //       "text": "<p>Siri is Apple's virtual assistant. It uses voice queries, gesture based control, focus-tracking and a natural-language user interface to answer questions, make recommendations, and perform actions by delegating requests to a set of Internet services. The software adapts to users' individual language usages, searches, and preferences, with continuing use. Returned results are individualized.</p>"
    //     }
    //   },
    //   {
    //     "media": {
    //       "url": "https://www.youtube.com/watch?v=mmQl6VGvX-c",
    //       "caption": "Intro to Google Knowledge Graph <a target=\"_blank\" href='https://www.youtube.com/watch?v=mmQl6VGvX-c'>credits</a>"
    //     },
    //     "start_date": {
    //       "year": "2012",
    //       "month": "5"
    //     },
    //     "text": {
    //       "headline": "\"Things, Not Strings\"",
    //       "text": "<p>Google introduces Knowlegde Graph in it's <a target=\"_blank\" href='https://blog.google/products/search/introducing-knowledge-graph-things-not/'>blog</a>. KG enhances Google Search in three main ways. By finding the right thing, getting the best summary and discovering deeper and broader information.</p>"
    //     }
    //   },
    //   {
    //     "media": {
    //       "url": "https://miro.medium.com/max/2400/1*YvOdGp73pOHmYGHKqkx5wQ.png",
    //       "caption": "Word2Vec Skip-Gram (<a target=\"_blank\" href='https://becominghuman.ai/how-does-word2vecs-skip-gram-work-f92e0525def4'>credits</a>)"
    //     },
    //     "start_date": {
    //       "year": "2013",
    //       "month": "2"
    //     },
    //     "text": {
    //       "headline": "Word2vec",
    //       "text": "<p>Tomas Mikolov at Google publishes Word2vec. Word2vec is a group of models used to express words with vectors. These models are two-layer neural networks designed to reconstruct the contexts of words.<p></p>Word2vec retrieves a text as input and generates a vector space from it, which can consist of hundreds of dimensions, each word in the text being assigned to a corresponding vector in space. Words that share common context in the text are positioned close to each other in the vector space.</p>"
    //     }
    //   },
    //   {
    //     "media": {
    //       "url": "https://images-na.ssl-images-amazon.com/images/I/612KpcXcFBL._AC_SL1000_.jpg",
    //       "caption": "Amazon Echo Dot (4rd gen) <a target=\"_blank\" href='https://images-na.ssl-images-amazon.com/images/I/612KpcXcFBL._AC_SL1000_.jpg'>credits</a>"
    //     },
    //     "start_date": {
    //       "year": "2014",
    //       "month": "11"
    //     },
    //     "text": {
    //       "headline": "Amazon Alexa",
    //       "text": "<p>Amazon Alexa is a virtual assistant developed by Amazon. It's AI technology is used in the Amazon Echo smart speakers. It is capable of voice interaction, music playback, making to-do lists, setting alarms, streaming podcasts, playing audiobooks, and providing weather, traffic, sports, and other real-time information, such as news. Alexa can also control several smart devices using itself as a home automation system.</p>"
    //     }
    //   },
    //   {
    //     "media": {
    //       "url": "https://spacy.io/architecture-415624fc7d149ec03f2736c4aa8b8f3c.svg",
    //       "caption": "spaCy Architecture <a target=\"_blank\" href='https://spacy.io/architecture-415624fc7d149ec03f2736c4aa8b8f3c.svg'>credits</a>"
    //     },
    //     "start_date": {
    //       "year": "2015",
    //       "month": "2"
    //     },
    //     "text": {
    //       "headline": "spaCy",
    //       "text": "<p>Industrial-Strength Natural Language Processing in Python</p><p>The initial release of spaCy from Explosion AI was in 2015. In 2021 the python/cython package for NLP has a version 3 with a whole ecosystem of plugins, components and workflows and support for 65+ languages.</p>"
    //     }
    //   },
    //   {
    //     "media": {
    //       "url": "https://upload.wikimedia.org/wikipedia/commons/c/cb/Google_Assistant_logo.svg",
    //       "caption": "Google Assistant logo <a target=\"_blank\" href='https://commons.wikimedia.org/wiki/File:Google_Assistant_logo.svg'>credits</a>"
    //     },
    //     "start_date": {
    //       "year": "2016",
    //       "month": "5"
    //     },
    //     "text": {
    //       "headline": "Google Assistant",
    //       "text": "<p>Google Assistant is an AI–powered virtual assistant developed by Google that is available on mobile and smart home devices. Google Assistant can engage in two-way conversations and is triggered with the word 'Hey Google'.</p>"
    //     }
    //   },
    //   {
    //     "media": {
    //       "url": "https://www.muraldecal.com/en/img/asy165-png/folder/products-detalle-png/wall-stickers-for-kids-bert-of-sesame-street.png",
    //       "caption": "Bert <a target=\"_blank\" href='https://www.muraldecal.com/en/img/asy165-png/folder/products-detalle-png/wall-stickers-for-kids-bert-of-sesame-street.png'>credits</a>"
    //     },
    //     "start_date": {
    //       "year": "2018",
    //       "month": "10"
    //     },
    //     "text": {
    //       "headline": "BERT",
    //       "text": "<p>Bidirectional Encoder Representations from Transformers (BERT) is a Transformer-based machine learning technique for natural language processing (NLP) pre-training developed by Google.</p>"
    //     }
    //   },
    //   {
    //     "media": {
    //       "url": "https://miro.medium.com/max/700/1*YfDk5y2DORltfeIVEAEdNg.png",
    //       "caption": "Huggingface logo <a target=\"_blank\" href='https://medium.com/nerd-for-tech/nlp-with-hugging-face-transformers-a41caadf6f2'>credits</a>"
    //     },
    //     "start_date": {
    //       "year": "2018",
    //       "month": "11"
    //     },
    //     "text": {
    //       "headline": "Huggingface Transformers",
    //       "text": "<p>Launched in november 2018 and 40k stars on Github in Januari 2021!</p><p>State-of-the-art Natural Language Processing for Pytorch and TensorFlow 2.0. Transformers provides general-purpose architectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNet…) for Natural Language Understanding (NLU) and Natural Language Generation (NLG) with over 32+ pretrained models in 100+ languages and deep interoperability between TensorFlow 2.0 and PyTorch. 🤗</p>"
    //     }
    //   },
    //   {
    //     "media": {
    //       "url": "https://res.infoq.com/news/2020/06/openai-gpt3-language-model/en/headerimage/openai-gpt3-language-model-1590942288273.jpg",
    //       "caption": "<a target=\"_blank\" href='https://res.infoq.com/news/2020/06/openai-gpt3-language-model/en/headerimage/openai-gpt3-language-model-1590942288273.jpg'>credits</a>"
    //     },
    //     "start_date": {
    //       "year": "2020",
    //       "month": "6"
    //     },
    //     "text": {
    //       "headline": "GPT-3",
    //       "text": "<p>Generative Pre-trained Transformer 3 (GPT-3) is an autoregressive language model that uses deep learning to produce human-like text. It is created by OpenAI and has 175 billion parameters.<p></p>The New York Times wrote in July 2020 that GPT-3 —which can generate computer code and poetry, as well as prose— is not just 'amazing, spooky, and humbling', but also 'more than a little terrifying'.</p>"
    //     }
    //   },
    //   {
    //     "media": {
    //       "url": "https://www.pngkey.com/png/full/86-862597_back-to-the-future-delorean-clipart-back-to.png",
    //       "caption": "Delorean Time Machine from Back To The Future <a target=\"_blank\" href='https://www.pngkey.com/png/full/86-862597_back-to-the-future-delorean-clipart-back-to.png'>credits</a>"
    //     },
    //     "start_date": {
    //       "year": "2030"
    //     },
    //     "text": {
    //       "headline": "The Future?",
    //       "text": "<p>Analysing text for low-resource domains and languages, with high transparency and explainability and non-biased and socially responsible models?</p><p>Visit innerdoc.com for more articles about Natural Language Processing!</p>"
    //     }
    //   }
    // ]
}